#
# Copyright (c) Fortanix, Inc.
#
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#
# Overview of test_app container classes:
#
# * AppTestContainer, a common base class, which includes some docker
#   utilities since even for kubernetes runs we need to use docker to push
#   images.
# * DockerContainer, which implements `run` to start a docker created by
#   `prepare_container`, along with the routines related to log extraction and
#   determining container network address
# * NativeContainer, which inherits from DockerContainer, and implements
#   `prepare_image` to pull a docker image without conversion, and
#   `prepare_container` to create (but not run) a docker container for
#   a native image
# * ZirconContainer, which implements `prepare_image` to invoke the converter
# * ZirconDockerContainer, which inherits from ZirconContainer and
#   DockerContainer, and implements `prepare_container` to create (but not run) a
#   docker container for a zircon converted image
# * KubeContainer, which implements `run` to launch a kubernetes pod spec
#   generated by `prepare_container`, along with the routines related to log
#   extraction and determining network address
# * NativeKubeContainer, which inherits from KubeContainer, and implements
#   `prepare_image` to pull a docker image and then push it to the registry used
#   by the kubernetes cluster, and `prepare_container` to write a pod spec
#   for a native image
# * ZirconKubeContainer, which inherits from KubeContainer and ZirconContainer,
#   and implements `prepare_container` to write a pod spec for a zircon converted
#   image
#
# The kubernetes implementation does not have feature parity with the docker
# implementation, and many app tests will not work on kubernetes.

import argparse
import csv
import docker
import io
import kubernetes
import os
import pexpect
import random
import re
import requests
import signal
import stat
import string
import subprocess
import sys
import tarfile
import time
import traceback
from pexpect import fdpexpect
from test_utils import (TestException, TestResults, TimeoutException,
                        remove_glob, remove_ignore_nonexistent,
                        zircon_panic_msg_binary)

if os.environ.get('IS_NITRO'):
    class Object:
        pass

    string_table = Object()

    string_table.PACKAGE_INSTALL_ROOT = "/"
else:
    import string_table

import json
# See https://github.com/pexpect/pexpect/issues/351. pexpect 4.0 and 4.1 do
# not correctly implement non-blocking I/O in fdpexpect.
from distutils.version import StrictVersion
from types import SimpleNamespace

if StrictVersion(pexpect.__version__) < StrictVersion('4.2.0'):
    print('Your pexpect is too old (you have {}; 4.2.0 or newer is required)'.format(StrictVersion(pexpect.__version__)))
    print('Try running the latest zircon chef recipe')
    sys.exit(1)

scriptDirectory = os.path.dirname(os.path.realpath(__file__))

DEB_PKG_INSTALL_BASE = string_table.PACKAGE_INSTALL_ROOT

# Default overall test timeout.
DEFAULT_TEST_TIMEOUT = 600
# Default timeout for each expect() call. (pexpect is deprecated, do not use in new tests)
PEXPECT_TIMEOUT = 480
# Default timeout for stopping a single test container.
DEFAULT_STOP_TIMEOUT = 300

# Factor to increase timeouts for SGX. This previously was 3, but when we run without wrfsbase support,
# worst-case time is about 3 times longer than that, so it was increased to 9.
SGX_TIMEOUT_SCALE = 9

DOCKER_REGISTRY = os.environ['DOCKER_REGISTRY']
SMARTKEY_ENDPOINT = 'https://amer.smartkey.io/'
IBMCLOUD_REGISTRY = 'us.icr.io/datashield-dev'

# Some tests run with a stock ubuntu container.
BASE_UBUNTU_CONTAINER = 'ubuntu'
BASE_UBUNTU_VERSION = 'xenial-20181113'

# For apps which are configured to send heartbeats, the expected heartbeat count can
# fall within an acceptable range instead of being an exact value. The variation depends
# on factors like system load. This variable is added/subtracted from the expected heartbeat
# count to obtain the expected range.
EXPECTED_HEARTBEAT_DELTA = 2

# All containers that we created during this test, so we can clean them up. Each new container is added to the
# end of the list. We clean up containers in last-in-first-out order, so when cleaning up, we start from the
# end and work to the beginning.
all_containers = []

# All additional container images that we created during this test. Some tests exercise re-signing converted
# containers with the signing tool. This produces additional images that need to be cleaned up.
all_images = []


# Whether anything happend during the test that means we should consider the test run a failure.
failed = False

running_test_name = None

# TODO: Support tests that need more than one network.
container_network = None

# Used for generating unique names for each container. Not necessarily a count of how many containers
# are currently running for this test.
container_count = 0

# TODO: docker requires that the names of objects be unique (for that type of object). It's unlikely
# but possible that this random number could collide with another set of tests running simultaneously.
# We don't need to worry about this too much because with the automated CI testing, we only
# run one set of tests at a time per host.
test_unique_name = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(14))

log_dir = 'logs'

# Exception for signing failures. Used for signer tests.
class SignerException(Exception):
    pass

def is_sgx():
    return os.environ.get('PLATFORM', 'linux') == 'sgx'

def create_log_dir():
    if not os.path.exists(log_dir):
        try:
            os.mkdir(log_dir, 0o744)
        except OSError as e:
            if e.errno == errno.EEXIST and os.path.isdir(log_dir):
                pass
            else:
                print(e)
                print('Error creating log directory \'{}/\''.format(log_dir))
                sys.exit(1)
    else:
        remove_glob('{}/*'.format(log_dir))


def get_network(image, network_name = None):
    global container_network
    global test_unique_name
    if network_name is not None:
        docker_client = docker.from_env()
        for network in docker_client.networks.list():
            if network.name == network_name:
                container_network = network
                return network.id

    if container_network is None:
        unique = 'zircon-ci-' + test_unique_name + '--' + image
        unique = re.sub('[^-0-9a-zA-Z._]', '_', unique)

        docker_client = docker.from_env()
        # TODO: Handle duplicate network name (very unlikely).
        # TODO: Enable ipv6. I get an error: ("could not find an available, non-overlapping IPv6 address pool among
        # the defaults to assign to the network"). We probably need to specify a network configuration:
        # https://github.com/moby/moby/issues/29443. This may need to be passed via the ipam parameter to the
        # networks.create() call.
        network = docker_client.networks.create(unique, driver='bridge', check_duplicate=True, enable_ipv6=False)
        container_network = network

    return container_network.id

def destroy_network():
    global container_network
    if container_network is not None and container_network.name != 'bridge':
        try:
            container_network.remove()
        except docker.errors.NotFound:
            msg = "Warning: container network {} was deleted outside global_cleanup()."
            print(msg.format(container_network.id))
        finally:
            container_network = None

def gen_volume_name(container_name):
    return gen_hostname(container_name)+ '-persist_volume'

# Generate a unique legal host name. Docker will only provide DNS resolution for
# containers with DNS-legal names, and Kubernetes only allows DNS-legal names.
def gen_hostname(base_name):
    global container_count
    global test_unique_name
    global running_test_name

    test_name = running_test_name if running_test_name else 'eos-ci'

    new_name = 'eos-test-{}-{}-{}-{}'.format(test_unique_name, container_count, base_name, test_name.lower())
    container_count += 1

    # Filter out anything besides letters, numbers, and hyphens. Hyphens are
    # illegal at the start and end of the name, so filter that out too.
    new_name = re.sub('[^-0-9a-zA-Z]', '-', new_name)
    new_name = re.sub('(^-+)|(-+$)', '', new_name)
    new_name = new_name[:50]
    return new_name

def check_zircon_log_for_panic(logfile):
    ''' Returns True if the log contained a panic or False if it did not.'''
    print('checking logfile {} for panics'.format(logfile))
    with open(logfile, 'rb') as fh:
        context = []
        for line in fh:
            context.append(line)
            if len(context) > 100:
                context.pop(0)

            if zircon_panic_msg_binary in line:
                print('zircon log contains one or more panics. Failing test. Panic output:')
                if context:
                    sys.stdout.buffer.write(b''.join(context))
                sys.stdout.buffer.write(line.strip())
                i = 0
                for line in fh:
                    sys.stdout.buffer.write(line.strip())
                    i += 1
                    if i > 5:
                        break
                sys.stdout.buffer.write(b'\n')
                return True

            prev_line = line
    return False

def run_converter(conv_bin_cmd, image):
    print('Running converter command {}'.format(conv_bin_cmd))

    # Using the max number of global containers to name the image conversion
    # logs at this point since the container name is assigned later
    container_num = len(all_containers)
    conv_stdout = os.path.join(log_dir, '{}.conv.out'.format(str(container_num)))
    conv_stderr = os.path.join(log_dir, '{}.conv.err'.format(str(container_num)))
    print('Saving logs from converter for image {} in {} and {}'.format(
          image, conv_stdout, conv_stderr))

    start_time = time.time()

    # Save converter logs to file
    with open(conv_stdout, "wb") as out, open(conv_stderr, "wb") as err:
        conv_p = subprocess.Popen(conv_bin_cmd, shell=False, stdout=out, stderr=err)
        # Wait for the process to finish
        ret = conv_p.wait()

        # Flush the file handles so that all the data is written into the file
        # before we try to read from it
        out.flush()
        err.flush()

    # Log time taken for the image conversion
    print("CONVERT_TIME: {:.1f}".format(time.time() - start_time))

    return ret, conv_stdout, conv_stderr

class Logs(object):
    def __init__(self, stdout=[], stderr=[]):
        self._stdout = stdout
        self._stderr = stderr

    @property
    def stdout(self):
        return self._stdout

    @property
    def stderr(self):
        return self._stderr

    def __str__(self):
        return ('stdout:' + '\n' +
                '\n'.join(self._stdout) + '\n' +
                'stderr:' + '\n' +
                '\n'.join(self._stderr) + '\n')

# Get the SGX device driver nodes that need to be provided to the container to run.
# For now, we pass through any of the devices that are present. Note that in the case
# of gsgx, there's an auxv entry that tells us whether the kernel has native
# support for fsgsbase. However, there's no easy way to access the auxiliary vector
# from python, and we want to allow people to run without the gsgx driver even
# if their kernel doesn't have fsgsbase support. So we just check for the presence
# of each of these device nodes and run the test container with any that are present.
def get_sgx_devices():
    if not is_sgx():
        return []
    if os.environ.get('IS_NITRO'):
        return []

    present_devices = []

    possible_devices = [
        '/dev/isgx',
        '/dev/sgx',
        '/dev/sgx_enclave',
        '/dev/gsgx',
        '/dev/sgx/enclave',
        '/dev/sgx/provision',
    ]
    for device in possible_devices:
        try:
            statbuf = os.lstat(device)
            if stat.S_ISCHR(statbuf[stat.ST_MODE]):
                present_devices.append('{}:{}:rwm'.format(device, device))
        except Exception:
            pass

    if len(present_devices) == 0:
        raise TestException('Unable to find any SGX device nodes to provide to the test container')
    else:
        print('Test container will run with device nodes: {}'.format(present_devices))

    return present_devices


def compare_output(output, expected):
    found = True
    for el in expected:
        found_one = any(el in line.strip() for line in output)

        if not found_one:
            print(el + 'not found.\n')

        found = found and found_one

    if (found == False):
        print('Container output did not match expected. Output:\n')
        print(output)
        raise TestException('Output mismatch')
    return True

def get_ecr_region():
    f = open(os.path.expanduser('~') + "/.aws/config", "r")
    for line in f:
        if ('region' in line):
            r = line.split('=')[-1].strip()
            return r
    return None

def get_latest_image_tag(image_name, registry):
    if ('zapps' not in image_name):
        return 'latest'

    #
    region = registry.split('.')[3]
    print('region ' + region)
    command = "aws ecr describe-images --repository-name " + image_name + \
              " --region " + region + " --output text \
              --query 'sort_by(imageDetails,& imagePushedAt)[*].imageTags[*]' \
              | tr '\\t' '\\n' | grep 20.*-.* | sort -r | head -n 1" # we are fetching all the tags of the format
              # *-* because we tag images in zircon-apps as <timstamp>-<hash>. Timestamp begins
              # with the year

    latest_tag = os.popen(command).read().rstrip()
    print('latest tag fetched is: ' + latest_tag)
    return latest_tag


def get_default_image_tag(image_name, registry):
    return get_latest_image_tag(image_name, registry)

class AppTestContainer(object):
    MAX_LOG_LINES = 10000

    def __init__(self, image, registry=DOCKER_REGISTRY,
                 image_version=None, manifest_options=None,
                 nitro_memsize=None, memsize=None, stacksize=None, thread_num=24,
                 manifest_env=None, entrypoint=None, run_args=None, auto_remove=True,
                 converted_image=None, encrypted_dirs=None,
                 persistent_volume=None, network_mode='bridge', ports=None,
                 pexpect=False, pexpect_tmo=PEXPECT_TIMEOUT, certificates=None,
                 ca_certificates=None, hostname=None, name=None, user=None,
                 app=None, java_mode=None, rw_dirs=None, ro_dirs=None,
                 allow_all_env=None, allow_some_env=None, container_env={'ENCLAVEOS_DISABLE_DEFAULT_CERTIFICATE':'1'},
                 post_conv_entry_point=None, network=None, zircon_debug=None,
                 log_file_path=None, zircon_panic_expected=None,
                 expected_status=0, dirs_to_copy=None, skip_converter_version_check=False,
                 cpu_count=2, input_auth_config=None, output_auth_config=None,
                 allow_docker_pull_failure=False, allow_docker_push_failure=True,
                 enable_overlay_fs_persistence=None):

        # This is needed for kube containers as well as docker, to
        # push images.
        self.docker_client = docker.from_env()

        self.converted_image = converted_image
        self.image = image
        self.registry = registry
        self.image_version = image_version if image_version is not None else \
            get_default_image_tag(self.image, self.registry)
        self.input_auth_config = input_auth_config
        self.output_auth_config = output_auth_config
        self.manifest_options = manifest_options
        self.memsize = memsize
        self.stacksize = stacksize
        self.thread_num = thread_num

        # Cpu count and use_filesystem is supported only by Nitro
        self.nitro_memsize = nitro_memsize
        self.cpu_count = cpu_count
        self.allow_docker_push_failure = allow_docker_push_failure
        self.enable_overlay_fs_persistence = enable_overlay_fs_persistence

        # These environment variables are added just to the application's manifest.
        self.manifest_env = manifest_env
        # These environment variables will be passed to the container at runtime but not added
        # to the application's manifest. They can be used with the allow_some_env input param
        # to pass through a set of allowed env variables at runtime.
        self.container_env = container_env
        if (self.container_env is not None):
            # For testing we disable fetching of default certs by default

            # ToDo ZIRC-5887 : For now some apptests pass container_env
            # as a dict, some as a list and some as a set. We should fix this.
            if (str(type(self.container_env)) == "<class 'dict'>" and
                'ENCLAVEOS_DISABLE_DEFAULT_CERTIFICATE' not in self.container_env):
                self.container_env ['ENCLAVEOS_DISABLE_DEFAULT_CERTIFICATE'] = '1'
            elif (str(type(self.container_env)) == "<class 'set'>" and
                 not any(x.startswith('ENCLAVEOS_DISABLE_DEFAULT_CERTIFICATE') for x in self.container_env)):
                self.container_env.add('ENCLAVEOS_DISABLE_DEFAULT_CERTIFICATE=1')
            elif (str(type(self.container_env)) == "<class 'list'>" and
                 not any(x.startswith('ENCLAVEOS_DISABLE_DEFAULT_CERTIFICATE') for x in self.container_env)):
                self.container_env.append('ENCLAVEOS_DISABLE_DEFAULT_CERTIFICATE=1')

        self.entrypoint = entrypoint
        # TODO: put some more thought into an appropriate generic mechanism
        # for combining run_args and test-specified args. see also
        # TestApp.container
        self.run_args = run_args
        self.security_opt = self.run_args.security_opt if self.run_args else None
        self.encrypted_dirs = encrypted_dirs
        self.rw_dirs = rw_dirs
        self.ro_dirs = ro_dirs
        self.dirs_to_copy = dirs_to_copy
        self.post_conv_entry_point = post_conv_entry_point
        # dictionary with name:mountpath pair for docker volume
        self.persistent_volume = persistent_volume
        self.network_mode = network_mode
        self.ports = ports
        self.pexpect = None
        self.certificates = certificates
        self.ca_certificates = ca_certificates
        self.app = app
        self.hostname = hostname
        self.name = name or gen_hostname(os.path.basename(self.image))
        self.user = user
        self.network = network
        self.java_mode = java_mode
        self.allow_all_env = allow_all_env
        self.allow_some_env = allow_some_env
        self.allow_docker_pull_failure = allow_docker_pull_failure
        self.zircon_debug = zircon_debug
        self.log_file_path = log_file_path
        self.zircon_panic_expected = zircon_panic_expected
        self.expected_status = expected_status
        self.skip_converter_version_check = skip_converter_version_check

        # Tests that don't want to use pexpect can hang if they're not careful to clear the pexpect sockets
        # periodically. ZIRC-1166.
        self.use_pexpect = pexpect
        self.pexpect_tmo = pexpect_tmo
        self.log_output_times = 0
        # Converted image json object containing mrenclave, mrsigner, isvsvn, isvprodid
        self.converted_image_attributes = None
        self.mod_img = None
        all_containers.append(self)

    def expect(self, regex, **kwargs):
        if self.pexpect:
            return self.pexpect.expect(regex, **kwargs)
        else:
            TestException("pexpect not enabled for container {}".format(self.name));
            return None

    def dump_output(self):
        if self.use_pexpect:
            print('\n--- Container output:')
            output = self.pexpect.before.decode(errors='replace')
            print(output, end='')
            if len(output) > 0 and output[-1] != '\n':
                print('\n')
            print('--- End container output')

    # This is generally less useful than just dumping the container output
    def dump_expect(self):
        if self.pexpect:
            print(str(self.pexpect))

    # Subclasses can override this method to save the zircon log file.
    def save_log_file(self):
        pass

    def run_and_get_logs(self, rerun=False):
        if rerun:
            self.rerun()
        else:
            self.run()
        self.wait()
        return self.logs()

    # This searches only Z_LOG_FATAL Z_LOG_ERROR messages as they are the only
    # ones that get printed to the stderr of the docker container.
    def search_one_line(self, logs, expected):
        for err in logs.stderr + logs.stdout:
            if expected in err:
                print('Found expected text in logs: ' + '\'{}\''.format(expected))
                return True
        else:
            print('\nContainer output did not contain expected. Output printed above\n')
            print('Expected:\n')
            print(expected)
            print('\n')
            raise TestException('Output mismatch')

    """ rerun and cleanup are optional parameters to the following 4
        functions which are set to False by default. It is upto the
        caller to use these as needed for testing apps. Note that once
        cleanup is performed, rerun of the same container will not be
        possible later in the test since the container would be cleaned
        up from the docker cache.
    """

    # Searches only one line in the output.
    def run_and_search_logs(self, expected, rerun=False, cleanup=False):
        logs = self.run_and_get_logs(rerun)
        print(logs)
        status = self.search_one_line(logs, expected)

        if cleanup:
            global_cleanup(status)

        return status

    def run_and_search_multiple_lines_logs(self, expected_lines, rerun=False, cleanup=False):
        count = len(expected_lines)
        logs = self.run_and_get_logs(rerun)

        if (count < 1):
            if cleanup:
                global_cleanup(True)
            return True
        num_lines_got = 0

        for line in expected_lines:
            try:
                if (self.search_one_line(logs, line)):
                    num_lines_got += 1
            except:
                # keep on searching all the lines. The lines that are not found
                # will be printed in search_one_line. One all the lines are
                # searched, we either return true ( if all the lines were found)
                # or we raise one single exception.
                pass
        if (num_lines_got != count):
            print('Container logs >> {}'.format(logs))
            raise TestException('All lines not found')
        else:
            print("All lines found\n")
            if cleanup:
                global_cleanup(True)
            return True
    def run_and_compare_stdout(self, expected, rerun=False, cleanup=False):
        logs = self.run_and_get_logs(rerun=rerun)
        status = compare_output(logs.stdout, expected)

        if cleanup:
            global_cleanup(status)

        return status

    def run_and_compare_stderr(self, expected, rerun=False, cleanup=False):
        logs = self.run_and_get_logs(rerun=rerun)
        status = compare_output(logs.stderr, expected)

        if cleanup:
            global_cleanup(status)

        return status

    def run_and_return_logs(self):
        """ Run the container, wait for it to finish, and return the logs. """
        self.run()
        self.wait()
        return self.logs()

    def should_keep_container(self):
        global failed
        return ((failed and self.run_args and self.run_args.keep_containers_on_failure) or
                (not failed and self.run_args and self.run_args.keep_containers_on_success))

    def prepare(self, extra_args=None):
        if not self.converted_image:
            self.prepare_image()
        self.prepare_container(extra_args)

    def run(self):
        raise NotImplementedError

    def rerun(self):
        raise NotImplementedError

    def wait(self):
        raise NotImplementedError

    def logs(self):
        raise NotImplementedError

    def prepare_image(self):
        raise NotImplementedError

    def prepare_container(self, extra_args=None):
        raise NotImplementedError

    def stop(self, remove=True):
        return None

    def waited_delete(self, rtype, name):
        removed = False
        iters = 0
        while not removed and iters < 100:
            iters += 1
            try:
                if rtype == 'image':
                    self.docker_client.images.remove(name)
                    print('Remove docker image {}'.format(name))
                elif rtype == 'volume':
                    self.docker_client.volumes.get(name).remove(True)
                    print('Remove docker volume {}'.format(name))
                else :
                    print('Invalid type for waited_delete')
                removed = True
            except Exception:
                time.sleep(0.1)

class DockerContainer(AppTestContainer):
    """ Mixin for running containers with docker. Cannot be instantiated
        directly, needs something to provide an image. Inherits from
        AppTestContainer because it accesses AppTestContainer members, but
        that could probably be avoided with some additional refactoring. """

    def __init__(self, image, **kwargs):
        super().__init__(image, **kwargs)

    # Prepare keyword arguments for constructing a docker container that are common to all DockerContainer variants.
    def _prepare_common_kwargs(self):
        kwargs = {}

        if self.ports:
            kwargs['ports'] = self.ports

        if self.security_opt:
            kwargs['security_opt'] = self.security_opt

        if self.user:
            kwargs['user'] = self.user

        if self.hostname:
            kwargs['hostname'] = self.hostname

        kwargs['auto_remove'] = False
        kwargs['name'] = self.name

        if self.run_args and self.run_args.privileged:
            kwargs['privileged'] = True
        if self.run_args and self.run_args.host_pid_namespace:
            kwargs['pid_mode'] = 'host'

        return kwargs


    def copy_to_input_image(self, src_files: list, dest_dir):

        # When copying multiple source files, each source file is
        # separated by a space
        src_file_str = " ".join(src_files)

        # The base image used in the dockerfile is the original input image by default.
        # If we have already copied files into the image, then we use mod_img as the base
        # image
        input_img_ref = self.registry + '/' + self.image + ':' + self.image_version
        if self.mod_img:
            input_img_ref = self.mod_img

        # Use the current working directory, which is also the test build directory
        # as the build context directory. Place the new dockerfile here.
        test_build_dir = os.getcwd()
        with open(test_build_dir + '/' + 'Dockerfile' ,'wb') as out:
            out.write(('FROM {}\n'.format(input_img_ref)).encode('utf-8'))
            out.write(('COPY {} {}\n'.format(src_file_str, dest_dir)).encode('utf-8'))

        # Provide the path to the temp_dir. This has the Dockerfile and the build context i.e.
        # other files like dirs-to-check needed by the docker client to build this image
        (mod_img, logs) = self.docker_client.images.build(path=test_build_dir)
        self.mod_img = mod_img.short_id

        # These intermediate images will be cleaned up at the end of the test run
        all_images.append(self.mod_img)

        # Print logs to console
        for val in logs:
            # Check for stream index in logs, they contain the console output when
            # the client builds the docker image
            if 'stream' in val:
                print(val['stream'])

    # Convenience function to copy a single file or directory into the
    # container. The location in the container is determined by destdir
    # and the filename component of srcfile (any path on srcdir is used
    # only to identify the source).
    def copy_file(self, srcfile, destdir):
        self.copy_files([srcfile], destdir)

    def copy_files(self, src_files: list, dest_dir):
        archive = io.BytesIO()
        with tarfile.open(mode='w|', fileobj=archive) as tar:
            for src in src_files:
                tar.add(src, arcname=os.path.basename(src))
        self.copy_tar_files(archive.getvalue(), dest_dir)

    # Copy a tarball into the container
    def copy_tar_files(self, tar, destdir):
        # this is the only primitive docker gives us; consider copy_file or copy_files as helpers.
        self.container.put_archive(destdir, tar)

    # Copy a file from the container to wherever this script is running. srcfile must be an absolute path.
    def copy_file_from_container(self, srcfile, destfile):
        # Certain tests, when marked for auto-remove, trigger what seems to be a docker server bug (ZIRC-1023):
        # docker.errors.APIError: 500 Server Error: Internal Server Error ("Error: driver aufs is returning inconsistent paths for container 74fa66af4846bb3319278a889b00d8547060fe86e258ba2a76a058497782c06f ('/var/lib/docker/aufs/mnt/5b614ce05526d3acc801924bf19ebe15d14610721d0438e0b46ca40cd88cfff5' then '/var/lib/docker/aufs/diff/5b614ce05526d3acc801924bf19ebe15d14610721d0438e0b46ca40cd88cfff5')")
        #
        # This happens on the workdir test, which exits on its own without needing to be stopped. So possibly this
        # bug occurs because of a race between the container being removed and trying to copy files from the container.
        # To work around this, we do not pass auto_remove when constructing the container and instead have logic
        # to explicitly remove the container when the tests complete.
        #
        print('Copying file {} from container'.format(srcfile))
        (response, _) = self.container.get_archive(srcfile)
        fileobj = io.BytesIO(b''.join(response))
        archive = tarfile.open(fileobj=fileobj)
        contents = archive.extractfile(os.path.basename(srcfile))
        with open(destfile, 'wb') as fh:
            fh.write(contents.read())

    def copy_dir_from_container(self, srcdir, destdir):
        print('Copying dir {} from container'.format(srcdir))
        (response, _) = self.container.get_archive(srcdir)
        fileobj = io.BytesIO(b''.join(response))
        archive = tarfile.open(fileobj=fileobj)

        cwd = os.getcwd()
        os.chdir(destdir)
        try:
            archive.extractall()
        except Exception:
            raise
        finally:
            os.chdir(cwd)

    def run(self):

        """ Start the container in the background. """

        print('Running container')


        # The socket should be attached before starting the container. If you start the container first, you might
        # lose some output by the time the socket is attached.
        if self.use_pexpect:
            self.sock = self.container.attach_socket(params={'stdout': 1, 'stderr': 1, 'stream': 1})

        if self.run_args and self.run_args.debug_shell:
            # There are some issues with giving the user an interactive shell in the container. The docker python
            # module doesn't support this without additional help. Some work is necessary to handle stdin/stdout/stderr
            # passthrough from the python process to the shell. There is a dockerpty python module to handle this
            # (at https://github.com/d11wtq/dockerpty), but dockerpty requires a newer version of the docker python
            # module than the one we use. The newer version of the docker module doesn't work in our environment
            # (possibly because we're using an old version of the docker engine). To avoid these issues, we exec
            # the docker start command that runs the container. This directly hands off stdin/stdout/stderr to the
            # container. It also makes it so the rest of the test script does not run once the container starts.
            # That behavior is either good or bad depending on how you want to debug your application.
            # TODO: There is no provision for debugging one container in a multi-container test.
            print('--debug-shell option detected')
            print('You will get a root shell in the converted container so you can debug')
            print('When you exit this shell, the test will automatically terminate')
            print('If you need to run a client against the running application')
            print('you will need to do that manually.')
            if sys.stdin.isatty() and not sys.stdout.isatty():
                # If stdin is a TTY and stdout is not a TTY, we're probably
                # running under make with the output piped to the make log.
                # For the debug shell, having stdout go to the TTY is probably
                # preferred. The output is available from `docker logs` if
                # needed.
                os.dup2(sys.stdin.fileno(), sys.stdout.fileno())
                os.dup2(sys.stdin.fileno(), sys.stderr.fileno())
            os.execl('/usr/bin/docker', '/usr/bin/docker', 'start', '-ai', self.container.short_id)
        else:
            self.container.start()
            if self.use_pexpect:
                self.pexpect = pexpect.fdpexpect.fdspawn(self.sock, timeout=self.pexpect_tmo)

    def restart_with_poll(self):
        # 150 retries with 2 seconds gap is like 5 mins in worst case

        retries = 150
        count = 1;

        # we keep on trying to start the container. Sometimes we get
        # docker.errors.APIError: 500 Server Error: Internal Server Error ("id already in use")
        # So we keep on trying till the container stops.
        while (count <= retries):
            try:
                if self.use_pexpect:
                    self.sock = self.container.attach_socket(params={'stdout': 1, 'stderr': 1, 'stream': 1})
                print('Restarting container (attempt ' + str(count) + ')')
                self.container.start()
                break
            except:
                time.sleep(2)
                count += 1
                if (count > retries):
                    raise Exception("Unable to stop and restart container. Possible causes system overload or docker daemon related issues")


    def rerun(self):
        print('Stopping container for rerun')
        if self.container is not None:
            self.container.stop()
            self.save_log_file()

        self.restart_with_poll()

        if self.use_pexpect:
            self.pexpect = pexpect.fdpexpect.fdspawn(self.sock, timeout=self.pexpect_tmo)

    def wait(self, expected_status=None, ignore_status=False):
        exp_status = self.expected_status
        if expected_status:
            exp_status = expected_status
        status = self.container.wait()['StatusCode']
        if not ignore_status and status != exp_status:
            raise TestException('Container returned unexpected exit status {}, expected {}'.format(
                status, exp_status))

    # Returns the last `lines` lines of output by the container on stdout.
    # Default for `lines` is `self.MAX_LOG_LINES`.
    def logs(self, lines=None):
        if lines is None:
            lines = self.MAX_LOG_LINES
        stdout = self.container \
            .logs(stdout=True, stderr=False, timestamps=False, tail=lines) \
            .decode('utf-8').rstrip().split('\n')
        stderr = self.container \
            .logs(stdout=False, stderr=True, timestamps=False, tail=lines) \
            .decode('utf-8').rstrip().split('\n')
        return Logs(stdout=stdout, stderr=stderr)

    def save_container_output(self, print_to_console=False):
        if not self.use_pexpect:
            try:
                # Here, we try to read the last MAX_LOG_LINES from the container's log. This may be insufficient
                # for debugging problems, but if the container creates a lot of output, then dumping all of the output
                # may take enormous amounts of time and space.
                print('Saving logs from container {}, print_to_console={}'.format(self.container.name, print_to_console))
                stdout_filename = '{}/{}.stdout.{}'.format(log_dir, self.container.name, self.log_output_times)
                with open(stdout_filename, 'wb') as container_stdout:
                    out_str = self.container.logs(stdout=True, stderr=False, timestamps=True, tail=self.MAX_LOG_LINES)
                    container_stdout.write(out_str)
                stderr_filename = '{}/{}.stderr.{}'.format(log_dir, self.container.name, self.log_output_times)
                with open(stderr_filename, 'wb') as container_stderr:
                    err_str = self.container.logs(stdout=False, stderr=True, timestamps=True, tail=self.MAX_LOG_LINES)
                    container_stderr.write(err_str)
                if print_to_console:
                    sys.stdout.write('\nStart of logs from container {}\n'.format(self.container.name))
                    sys.stdout.buffer.write(err_str[:5000])
                    sys.stdout.write('\nEnd of logs from container {}\n'.format(self.container.name))
                    sys.stdout.buffer.write(err_str[-5000:])
            except Exception as e:
                print('Exception trying to get container output: {}'.format(e))
                traceback.print_exc()
            finally:
                self.log_output_times += 1

    def stop(self, remove=True, status=True):
        logfile = None
        if self.container is not None:
            print('Stopping container {} status={}'.format(self.container.name, status))
            self.container.stop()
            logfile = self.save_log_file()
            # If test status is false, then print logs to console
            self.save_container_output(not status)

            if remove and not self.should_keep_container():
                self.container.remove(force=True, v=True)

                if self.persistent_volume:
                    for k, v in self.persistent_volume.items():
                        self.waited_delete(rtype='volume', name=k)

            # Remove this container from the list of containers so we don't (unnecessarily) try to stop it again.
            if remove and self in all_containers:
                all_containers.remove(self)
        super().stop(remove)
        return logfile

    # Return information about ports remapped to the host. For now, assume TCP.
    def get_port_mapping(self, port):
        # Inspect isn't available from the regular docker API, so we have to use the low level API.
        llc = docker.APIClient(base_url='unix://var/run/docker.sock')
        info = llc.inspect_container(self.container.name)
        portstr = '{}/tcp'.format(port)
        if 'NetworkSettings' in info and 'Ports' in info['NetworkSettings'] and \
           portstr in info['NetworkSettings']['Ports']:
            return int(info['NetworkSettings']['Ports'][portstr][0]['HostPort'])

        else:
            raise TestException('No port mapping found for port {}/tcp'.format(port))
    # Get IP information on the container network
    def get_my_ip(self, network_mode='bridge'):
        if self.network_mode == 'bridge':
            global container_network
            network_name = container_network.name
            llc = docker.APIClient(base_url='unix://var/run/docker.sock')
            info = llc.inspect_container(self.container.name)
            ip = None
            try:
                ip = info['NetworkSettings']['Networks'][network_name]['IPAddress']
            except:
                #Keep this comment to debug in case the json format change
                print(info)
                print(info['NetworkSettings']['Networks'])
                raise
            return ip
        elif self.network_mode == 'none':
            return None
        else:
            # TODO handle other network
            return '127.0.0.1'

    def configure_network_mode(self, container_kwargs):
        if self.network is not None:
            network_id = get_network(None, network_name=self.network)
            container_kwargs['network'] = network_id
        else:
            if self.network_mode in ['host', 'none'] or self.network_mode.startswith('container:'):
                container_kwargs['network_mode'] = self.network_mode
            elif self.network_mode == 'bridge':
                network_id = get_network(self.image)
                container_kwargs['network'] = network_id
            else:
                raise TestException('Unrecognized network mode {} requested', self.network_mode)

    def check_CA_trust_store(self, trust_store_path, ca_cert_data):
        status = False
        try:
            local_copy = './trusted_ca_list'
            self.copy_file_from_container(trust_store_path, local_copy)
            with open(local_copy, 'rb') as f:
                if ca_cert_data.encode('utf-8') in f.read():
                    print('CA cert was successfully installed in the trust store of {}\n'.format(self.container.name))
                    status = True
                else:
                    print('CA cert was not installed in the trusted store of {}\n'.format(self.container.name))
            remove_ignore_nonexistent(local_copy)
        except Exception as e:
            # traceback.print_exc()
            print('CA cert was not installed in the trusted store of {}: {}\n'.format(self.container.name, e))
        finally:
            remove_ignore_nonexistent(local_copy)
            return status

    def get_converted_image_attributes(self, successful_conv_prefix_str, conv_out):
        # Read the converter stdout logs line by line and search
        # for the converted image attributes. Assumption made here is that
        # all the attributes will be on one line. If the converter changes
        # the way it logs this information, we might need to update this
        # logic to look for a json object in the file
        prefix_str_index = -1
        with open(conv_out, "r") as rf:
            line = rf.readline()
            while line:
                prefix_str_index = line.find(successful_conv_prefix_str)
                if prefix_str_index >= 0:
                    break
                line = rf.readline()
        if (prefix_str_index < 0):
            raise ValueError('Failed to find converted image attributes')
        print('Full line: {}\n'.format(line))
        print('JSON line: {}\n'.format(line[line.find(successful_conv_prefix_str) + len(successful_conv_prefix_str):]))
        self.converted_image_attributes = json.loads(line[line.find(successful_conv_prefix_str)
                                                          + len(successful_conv_prefix_str):])
        print('printing attributes {}'.format(self.converted_image_attributes))


class KubeContainer(AppTestContainer):
    """ Mixin for running containers with kubernetes. Cannot be instantiated
        directly, needs something to provide an image. Inherits from
        AppTestContainer because it accesses AppTestContainer members, but
        that could probably be avoided with some additional refactoring. """

    def __init__(self, image, **kwargs):
        super().__init__(image, **kwargs)

        kubernetes.config.load_kube_config()

        self.api = kubernetes.client.apis.core_v1_api.CoreV1Api()

    def push_to_ibm_cloud(self, im, name, tag):
        repo = '{}/{}'.format(IBMCLOUD_REGISTRY, name)
        im.tag(repo, tag)
        print('Push to {}'.format(repo))
        for j in self.docker_client.images.push(repo, tag, stream=True, decode=True):
            #print(j)
            if ('error') in j:
                raise ValueError("could not push image, error " + j['error'])
        return '{}:{}'.format(repo, tag)

    def copy_file(self, srcfile, destdir):
        raise NotImplementedError

    def copy_file_from_container(self, srcfile, destfile):
        raise NotImplementedError

    def copy_dir_from_container(self, srcdir, destdir):
        raise NotImplementedError

    def dump_output(self):
        raise NotImplementedError

    def run(self):
        """ Start the container (kube pod) in the background. """

        print('Attempting to create test pod {} with image {}'.format(
            self.pod_manifest['metadata']['name'], self.pod_manifest['spec']['containers'][0]['image']))


        #if self.ports is not None:
        #    TODO set up kube services

        if self.use_pexpect:
            raise NotImplementedError

        if self.run_args.debug_shell:
            raise NotImplementedError

        resp = self.api.create_namespaced_pod(body=self.pod_manifest,
                                              namespace='default')
        self.container = self.pod_manifest['metadata']['name']

    def rerun(self):
        raise NotImplementedError

    def wait(self):
        while True:
            resp = self.api.read_namespaced_pod(name=self.container,
                                                namespace='default')
            if resp.status.phase not in ('Pending', 'Running', 'Unknown'):
                break
            time.sleep(1)

    def logs(self, lines=None):
        kwargs = {}
        if lines is not None:
            kwargs['tailLines'] = lines
        combined_log = self.api.read_namespaced_pod_log(name=self.container, namespace='default', **kwargs)
        return Logs(stdout=combined_log.splitlines(), stderr=[])

    def save_container_output(self):
        raise NotImplementedError

    def stop(self, remove=True):
        if self.container is not None:
            #TODO self.save_log_file()
            #self.save_container_output()
            if remove and not self.should_keep_container():
                print('Deleting pod {}'.format(self.container))
                self.api.delete_namespaced_pod(name=self.container, namespace='default')

            # Remove this container from the list of containers so we don't (unnecessarily) try to stop it again.
            if remove and self in all_containers:
                all_containers.remove(self)
        super().stop(remove)
        return None

class ZirconContainer(AppTestContainer):
    def __init__(self, image, **kwargs):
        super().__init__(image, **kwargs)

        self.enclaveos_log_count = 1


    def log_name_pattern(self, extension):
        return '{}/{}-enclaveos-log.{}'.format(log_dir, self.container.name, extension)



    def save_log_file(self):
        # Try to copy the enclave-os log file from the container. The log file might not exist if the test failed
        # before the log file was created, so ignore errors.
        output = self.log_name_pattern(self.enclaveos_log_count)
        srcfile = os.path.join(string_table.INSTALL_LOG_DIR, string_table.ZIRCON_LOG_FILE)

        # either log dir will be there or
        # one single log file will be there ( if log rotation is disabled)
        try:
            self.copy_file_from_container(srcfile, output)
            self.enclaveos_log_count += 1
            return output
        except Exception as e:
            if hasattr(e, 'message'):
                print(e.message)
            else:
                print(e)
            print('Unable to copy enclave-os.log from the container. Ignoring')

        try:
            log_dest_dir = log_dir + '/' + self.container.name
            os.mkdir(log_dest_dir)
            self.copy_dir_from_container(string_table.INSTALL_LOG_DIR, log_dest_dir )
            source_log_files = log_dest_dir + '/log/*/*'
            combined_log_file = output

            command = ' cat ' + source_log_files  + ' | sort -k 1 > ' + combined_log_file
            os.system(command)
            os.system('sed -i \'/^[[:space:]]*$/d\' ' + combined_log_file)
            self.enclaveos_log_count += 1
            return output
        except Exception as e:
            if hasattr(e, 'message'):
                print(e.message)
            else:
                print(e)
            print('Unable to copy enclave-os.log dir from the container. Ignoring')

    def prepare_image(self):
        extra_args = []
        if self.entrypoint:
            for arg in self.entrypoint:
                extra_args += ['--entrypoint={}'.format(arg)]
        if self.memsize:
            extra_args += ['--memsize', '{}'.format(self.memsize)]

        if self.stacksize:
            extra_args += ['--stacksize', '{}'.format(self.stacksize)]

        if self.thread_num:
            extra_args += ['--threads', '{}'.format(self.thread_num)]

        if self.encrypted_dirs:
            for d in self.encrypted_dirs:
                extra_args += ['--encrypted-dir', '{}'.format(d)]

        if self.rw_dirs:
            for d in self.rw_dirs:
                extra_args += ['--rw-dir', '{}'.format(d)]

        if self.ro_dirs:
            for d in self.ro_dirs:
                extra_args += ['--ro-dir', '{}'.format(d)]

        if self.certificates:
            extra_args += ['--certificates', '{}'.format(self.certificates)]

        if self.ca_certificates:
            extra_args += ['--ca-certificates', '{}'.format(self.ca_certificates)]
        if self.dirs_to_copy:
            extra_args += ['--dir-copy', '{}'.format(self.dirs_to_copy)]

        extra_args += ['--app', '{}'.format(self.app)] if self.app \
            else ['--app', '{}'.format(json.dumps({"app": {"heartbeat" : "false"}}))]

        if self.java_mode:
            extra_args += ['--java-mode', '{}'.format(self.java_mode)]

        if self.manifest_env:
            for e in self.manifest_env:
                extra_args += ['--manifest-env', '{}'.format(e)]

        if self.input_auth_config:
            extra_args += ['--input-auth-config', '{}'.format(self.input_auth_config)]

        if self.output_auth_config:
            extra_args += ['--output-auth-config', '{}'.format(self.output_auth_config)]

        imgname = [self.registry, '/', self.image, ':', self.image_version]
        if self.mod_img:
            imgname = self.mod_img

        cmd = ['{}'.format(self.run_args.toolserver)]
        cmd += ['--imagename',
                '{}'.format(''.join(imgname)),
                '--no-tag',
                '--write-image-id',
                'test-image-id',
                '--runtime-package',
                '{}'.format(self.run_args.installer),
                '--key',
                '{}'.format(self.run_args.key),
                '--isvsvn',
                '{}'.format(self.run_args.isvsvn),
                '--isvprodid',
                '{}'.format(self.run_args.isvprodid)]
        cmd += extra_args

        if self.zircon_debug:
            cmd += ['--debug']
        if self.log_file_path:
            cmd += ['--log-file-path', '{}'.format(self.log_file_path)]
        if self.run_args.allow_cmdline_arguments:
            cmd += ['--allow-cmdline-arguments']
        if self.run_args.toolserver_verbose:
            cmd += ['--verbose']
        if self.manifest_options:
            for (k, v) in self.manifest_options.items():
                cmd += ['--manifest-option={}={}'.format(k, v)]
        if self.run_args.manifest_option:
            for o in self.run_args.manifest_option:
                cmd += '--manifest-option={}'.format(o)
        if self.allow_all_env == '1':
            cmd += ['--manifest-option', 'loader.allow_all_env.all=1']
        if self.allow_some_env:
            for some_env in self.allow_some_env:
                cmd += ['--manifest-option', 'loader.allow_some_env.{}=1'.format(some_env)]

        cmd += ['--host-pkg-path', '{}'.format(os.environ['TEST_BASE_DIR'])]

        if not self.allow_docker_pull_failure:
            cmd += ['--allow-docker-pull-failure']

        try:
            status, conv_out, conv_err = run_converter(cmd, self.image)

        except subprocess.CalledProcessError as e:
            raise RuntimeError("command '{}' return with error (code {}): {}".format(e.cmd, e.returncode, e.output))

        if status != 0:
            print('Converter error logs:\n')
            print(open(conv_err, "r").read())
            raise TestException('Converter returned an error')

        # Read the converter stdout logs line by line and search
        # for the converted image attributes. Assumption made here is that
        # all the attributes will be on one line. If the converter changes
        # the way it logs this information, we might need to update this
        # logic to look for a json object in the file
        prefix_str_index = -1
        with open(conv_out, "r") as rf:
            line = rf.readline()
            while line:
                successful_conv_prefix_str = '[DONE] new image result is: '
                prefix_str_index = line.find(successful_conv_prefix_str)
                if prefix_str_index >= 0:
                    break
                line = rf.readline()
        if (prefix_str_index < 0):
            raise ValueError('Failed to find converted image attributes')
        self.converted_image_attributes = json.loads(line[line.find(successful_conv_prefix_str)
                                        + len(successful_conv_prefix_str):])
        print('printing attributes {}'.format(self.converted_image_attributes))

        with open('test-image-id', 'r') as fh:
            image = fh.read().strip()

        self.converted_image = image

    def stop(self, remove=True):
        if self.converted_image is not None:
            if not self.run_args.keep_converted_images:
                # Try to remove the converted image used for this test. The container was created with
                # auto_remove = true, so the Docker daemon will automatically remove the container when the
                # container stops. Unfortunately, we cannot remove the image until the daemon actually finishes
                # stopping the container. container.stop() only tells the daemon to stop the container. It does
                # not wait until the container is stopped. I don't see a way to wait until the container is
                # actually stopped. So retry removing the image for a while until it succeeds (or we time out).
                self.waited_delete(rtype='image', name=self.converted_image)

            else:
                print('Keeping original image {} as converted image {}'.format(self.image, self.converted_image))
        return super().stop(remove)


class NitroDockerContainer(DockerContainer):

    def __init__(self, *args, **kwargs):
        super(NitroDockerContainer, self).__init__(*args, **kwargs)
        self.input_image_ref = self.registry + '/' + self.image + ":" + self.image_version
        self.output_image_ref = self.image + ":" + "converted"
        self.container = None

    def prepare_image(self):
        request = Object()
        request.input_image = Object()
        request.output_image = Object()

        # If using intermediate modified image, we do not need to pass
        # auth credentials to the converter
        if self.mod_img:
            # Split the image id here to remove the sha256 prefix. The rust
            # docker library assumes sha256 is the image name and the digest
            # to be the tag which causes issues when creating intermediate
            # images during salmiac conversion.
            self.input_image_ref = (str(self.mod_img)).split("sha256:")[1]
        elif self.input_auth_config:
            request.input_image.auth_config = json.loads(self.input_auth_config)
        elif self.registry is DOCKER_REGISTRY and os.getenv("ECR_PASSWORD") is not None:
            print('Adding ftx ecr registry credentials in')
            request.input_image.auth_config = Object()
            request.input_image.auth_config.username = "AWS"
            request.input_image.auth_config.password = os.environ["ECR_PASSWORD"]

        if self.output_auth_config:
            request.output_image.auth_config = json.loads(self.output_auth_config)

        # We save output image into a local docker repo,
        # which means that we don't need output auth config
        request.nitro_enclaves_options = Object()
        request.converter_options = Object()

        request.input_image.name = self.input_image_ref
        request.output_image.name = self.output_image_ref

        request.nitro_enclaves_options.mem_size = self.memsize if self.nitro_memsize is None else self.nitro_memsize
        request.nitro_enclaves_options.cpu_count = self.cpu_count
        request.converter_options.push_converted_image = not self.allow_docker_push_failure

        #if os.getenv('ENCLAVEOS_DEBUG', "") == "debug" or os.getenv('FLAVOR', "") == "debug":
#            request.converter_options.debug = True

        if self.entrypoint:
            request.converter_options.entry_point = self.entrypoint

        if self.enable_overlay_fs_persistence:
            request.converter_options.enable_overlay_filesystem_persistence = self.enable_overlay_fs_persistence

        if self.certificates is not None:
            request.certificates = []

            for certificate in self.certificates:
                request.certificates.append(certificate)

        request_file_name = "converter-request.json"
        request_file = open(request_file_name, "w")

        # Run converter as root so that it can preserve filesystem permissions of
        # the input image while creating its blockfiles. SALM-329.
        cmd = ['sudo', '-E', self.run_args.toolserver]
        cmd += ['--request-file']
        cmd += [request_file_name]

        # Tells container converter not to delete output image
        # from local repository
        os.environ["PRESERVE_IMAGES"] = "result"

        try:
            conversion_req = json.dumps(request, default=lambda x: x.__dict__)
            print('CONVERSION REQ: {}\n'.format(conversion_req))
            request_file.write(conversion_req)
            request_file.close()
            ret, conv_out, conv_err = run_converter(cmd, self.image)

        except subprocess.CalledProcessError as e:
            raise RuntimeError("command '{}' returned with error (code {}): {}".format(e.cmd, e.returncode, e.output))

        if ret != 0:
            print('Converter error logs:\n')
            print(open(conv_err, "r").read())
            raise TestException('Converter returned an error')

        self.get_converted_image_attributes(successful_conv_prefix_str="Successful nitro conversion: ", conv_out=conv_out)

    def get_image_metadata(self):
        return self.converted_image_attributes


    def prepare_container(self, extra_args=None):
        container_kwargs = {}

        self.configure_network_mode(container_kwargs)

        container_kwargs['privileged'] = self.run_args.privileged
        container_kwargs['volumes'] = {
            "/run/nitro_enclaves": {
                "bind": "/run/nitro_enclaves",
                "mode": "rw"
            }
        }
        container_kwargs['environment'] = self.container_env

        self.container = self.docker_client.containers.create(self.output_image_ref, **container_kwargs, ports=self.ports)


# It is important that we inherit from DockerContainer first, because on
# cleanup we need to stop the container before we can delete the image.
class ZirconDockerContainer(DockerContainer, ZirconContainer):
    zone_cert = None
    def __init__(self, image, **kwargs):
        super(ZirconDockerContainer, self).__init__(image, **kwargs)

        self.image = image
        self.container = None

    def prepare_container(self, extra_args=None):
        print('Attempting to create test container from image {}'.format(self.converted_image))

        mounts = []
        if is_sgx():
            mounts += [
                {
                    'target': '/var/run/aesmd/aesm.socket',
                    'source': '/var/run/aesmd/aesm.socket',
                    'type': 'bind'
                },
            ]

        devices = get_sgx_devices()

        # Don't try to mount /dev/gsgx if the device node doesn't exist. There's an auxiliary vector entry that tells
        # us whether we're running on a kernel with fsgsbase support, but there's no simple way to access the auxiliary
        # vector from python. So just check for the existence of the node.
        if os.path.exists('/dev/gsgx'):
            devices.append('/dev/gsgx:/dev/gsgx:rwm')


        if self.persistent_volume:
            for k, v in self.persistent_volume.items():
                nv = {
                        'target':v,
                        'source':k,
                        'type': 'volume'
                     }
                mounts.append(nv)

        container_kwargs = self._prepare_common_kwargs()
        container_kwargs['mounts'] = mounts
        container_kwargs['devices'] = devices
        container_kwargs['environment'] = self.container_env
        container_kwargs['cap_add'] = [ 'SYS_PTRACE' ]
        if self.network_mode != 'host':
            # Disabling ipv6 may not be allowed on the host network, so only this option when it's not the host
            # network. ZIRC-4626.
            container_kwargs['sysctls'] = { 'net.ipv6.conf.all.disable_ipv6': '0' }

        self.configure_network_mode(container_kwargs)

        if self.run_args.debug_shell:
            container_kwargs['tty'] = True
            container_kwargs['entrypoint'] = ['/bin/sh', '/usr/local/bin/debug-shell-init.sh']
            container_kwargs['stdin_open'] = True
            container_kwargs['privileged'] = True
            # TODO: This sets it up so we run as root if we're getting a debug shell This is so we can set up
            # root's .gdbinit to load pal-gdb-sgx.py. It's hard to figure out what user the container is
            # normally going to run as.
            container_kwargs['user'] = 0
        elif self.post_conv_entry_point:
            # The post_conv_entry_point helps test cases that need to write scripts
            # which would run outside zircon, but would loop the converted app
            # within the convertor. The '/bin/sh' of the above debug_sell option
            # is also, in a way a post conversion entry point override, but
            # if debug_shell is also specified, we wouldn't want to go with
            # this override
            print("Overriding entry point to {}\n".format(self.post_conv_entry_point))
            container_kwargs['entrypoint'] = self.post_conv_entry_point

        self.container = self.docker_client.containers.create(self.converted_image, **container_kwargs)

        if self.run_args.debug_shell:
            self.copy_file(os.path.join(scriptDirectory, '..', '..', 'bin', 'debug-shell-init.sh'), '/usr/local/bin')
            if is_sgx():
                self.copy_file(os.path.join(scriptDirectory, '..', '..', 'bin', 'pal-gdb-sgx.py'), '/opt/fortanix/enclave-os')
                self.copy_file(os.path.join(scriptDirectory, '..', '..', 'gdb', 'dotgdbinit'), '/root')

        if self.converted_image:
            if not self.skip_converter_version_check:
                # Check that the converted container was correctly labeled. We don't perform this check if we're running
                # with a pre-converted container, since the version the container was converted with might be different
                # than the test tree we're running with.
                assert(string_table.CONVERTER_DOCKER_LABEL in self.container.labels)
                if not os.environ.get('RUNNING_FROM_MALBORK_CI'):
                    assert(self.container.labels[string_table.CONVERTER_DOCKER_LABEL] == string_table.PRODUCT_VERSION)


class ZirconKubeContainer(KubeContainer, ZirconContainer):
    def __init__(self, *args, **kwargs):
        super(ZirconKubeContainer, self).__init__(*args, **kwargs)

    def prepare_container(self):
        im = self.docker_client.images.get(self.converted_image)
        self.final_image = self.push_to_ibm_cloud(im, self.converted_image, 'latest')

        self.pod_manifest = {
            'apiVersion': 'v1',
            'kind': 'Pod',
            'metadata': {
                'name': self.name
            },
            'spec': {
                'restartPolicy': 'Never',
                'containers': [{
                    'name': 'app',
                    'image': self.final_image,
                    'volumeMounts': [{
                        'mountPath': '/dev/isgx',
                        'name': 'isgx',
                    },
                    {
                        'mountPath': '/dev/gsgx',
                        'name': 'gsgx',
                    },
                    {
                        'mountPath': '/var/run/aesmd/aesm.socket',
                        'name': 'aesm-socket',
                    }]
                }],
                'volumes': [{
                    'name': 'isgx',
                    'hostPath': {
                        'path': '/dev/isgx',
                        'type': 'CharDevice',
                    },
                },
                {
                    'name': 'gsgx',
                    'hostPath': {
                        'path': '/dev/gsgx',
                        'type': 'CharDevice',
                    },
                },
                {
                    'name': 'aesm-socket',
                    'hostPath': {
                        'path': '/var/run/aesmd/aesm.socket',
                        'type': 'Socket',
                    },
                }]
            }
        }

    def stop(self, remove=True):
        subprocess.check_call(['ibmcloud', 'cr', 'image-rm', self.final_image])
        return super().stop(remove)

class NativeContainer(DockerContainer):
    def __init__(self, *args, **kwargs):
        super(NativeContainer, self).__init__(*args, **kwargs)

    def prepare_image(self):
        self.final_image = self.registry + '/' + self.image + ":" + self.image_version
        # Try to pull the image from the remote registry. This might fail if you're running a test with
        # an image you have locally that hasn't been pushed to the registry yet, or if you're running tests when
        # you're not connected to the VPN. So ignore any errors but warn, in case the user gets unexpected behavior.
        try:
            self.docker_client.images.pull(self.registry + '/' + self.image, tag=self.image_version)
        except Exception:
            print('\n\n\n****** Unable to pull image {} ******'.format(self.final_image))
            print('Possible causes are the image does not exist on the registry,')
            print('a misspelled image name, or you are not connected to the VPN.')
            print('Attempting to run with local image\n\n\n')
            # deliberately do not reraise
        # We do not clean up the pulled image when the test finishes. The
        # pulled image is likely also the source image for some zircon test,
        # and we don't clean the pulled source images for zircon tests either.

    def prepare_container(self, extra_args=None):
        args = self._prepare_common_kwargs()

        args['environment'] = self.manifest_env
        args['mounts'] = [{'target': v, 'source': k, 'type': 'bind', 'RW': True} \
                          for k, v in self.persistent_volume.items()] if self.persistent_volume else None

        if extra_args:
            args.update(extra_args)

        self.configure_network_mode(args)

        if self.entrypoint:
            args['entrypoint'] = self.entrypoint

        self.container = self.docker_client.containers.create(self.final_image, **args)

class NativeKubeContainer(KubeContainer):
    def __init__(self, *args, **kwargs):
        super(NativeKubeContainer, self).__init__(*args, **kwargs)

    def prepare_image(self):
        # Try to pull the image from the remote registry. This might fail if you're running a test with
        # an image you have locally that hasn't been pushed to the registry yet, or if you're running tests when
        # you're not connected to the VPN. So ignore any errors but warn, in case the user gets unexpected behavior.
        try:
            im = self.docker_client.images.pull(self.registry + '/' + self.image, tag=self.image_version)
        except Exception:
            print('\n\n\n****** Unable to pull image {} ******'.format(self.registry + '/' + self.image + ':' + self.image_version))
            print('Possible causes are the image does not exist on the registry,')
            print('a misspelled image name, or you are not connected to the VPN.')
            print('Attempting to run with local image\n\n\n')
            # deliberately do not reraise
            im = self.docker_client.images.get(image)

        # TODO: do we want to worry about cleaning up the pulled copy of the image?
        # If so, it might make sense to combine with converted_image in some way.

        self.final_image = self.push_to_ibm_cloud(im, self.image, self.image_version)

    def prepare_container(self):
        self.pod_manifest = {
            'apiVersion': 'v1',
            'kind': 'Pod',
            'metadata': {
                'name': self.name
            },
            'spec': {
                'restartPolicy': 'Never',
                'containers': [{
                    'name': 'app',
                    'image': self.final_image,
                }],
            }
        }

        if self.entrypoint:
            self.pod_manifest['spec']['containers'][0]['command'] = self.entrypoint

        print(self.pod_manifest)

    def stop(self, remove=True):
        subprocess.check_call(['ibmcloud', 'cr', 'image-rm', self.final_image])
        return super().stop(remove)

# this function should be called if one of the calls to create_tar_for_copying
# fails. The reason is, if one calls to create_tar_for_copying fails, then some
# previously created tar files might interfere(and hence fail the test) with
# the future execution of the same test(unless the build directory is explicitly)
tar_files_created = []
def cleanup_tar_files_during_copying():
    for fn in tar_files_created:
        os.system('rm -rf ' + fn)

class TestApp:
    default_timeout_s = DEFAULT_TEST_TIMEOUT


    # Note: the test_arg_list parameter to the init function is no longer used for anything.
    def __init__(self, run_args, test_arg_list=None):
        self.run_args = run_args
        self.container_impl = CONTAINER_ENVS[run_args.container_env]
        self.toolserver = run_args.toolserver
        self.security_opt = run_args.security_opt
        self.results = TestResults(use_db=not run_args.no_results_db)

        global running_test_name
        running_test_name = self.__class__.__name__

        create_log_dir()

        if 'SOAK_JOB' in os.environ:
            self.soak = True
        else:
            self.soak = False

    # for every group of files or directories that need to be copied to one single
    # location, this function should be called once. The caller must make sure that
    # each time the function is called, the tar_name value is unique, else it will
    # replace some previously created tar file.
    def create_tar_for_copying(self, files, tar_name):
        if (os.path.exists(tar_name)):
            return False

        tar = tarfile.open(tar_name, mode='w')
        tar_files_created.append(tar_name)
        for fn in files:
            tar.add(fn)
        tar.close()

        return True

    def container(self, image: str, **kwargs):
        # TODO: put some more thought into an appropriate generic mechanism
        # for combining run_args and test-specified args. see also
        # AppTestContainer.__init__
        kwargs['run_args'] = self.run_args

        # memsize is required because (ZIRC-5538) it affects CI performance
        if 'memsize' not in kwargs and 'nitro_memsize' not in kwargs:
            raise TestException("memsize is a mandatory parameter")

        if 'converted_image' not in kwargs and self.run_args.converted_image:
            kwargs['converted_image'] = self.run_args.converted_image
        if 'post_conv_entry_point' not in kwargs and self.run_args.post_conv_entry_point:
            kwargs['post_conv_entry_point'] = self.run_args.post_conv_entry_point
        if 'zircon_debug' not in kwargs and self.run_args.zircon_debug:
            kwargs['zircon_debug'] = self.run_args.zircon_debug
        elif self.run_args.zircon_debug and kwargs['zircon_debug'] == False:
            print('Test has overridden --zircon-debug command-line option')

        return self.container_impl(image, **kwargs)

    @staticmethod
    def error(*args, **kwargs):
        print('Error:', *args, file=sys.stderr, **kwargs)

    @staticmethod
    def info(*args, **kwargs):
        print(*args, **kwargs)

    # Subclasses can override this method to set a different timeout. This method is an object method and not a class
    # method, so tests can set their timeout after performing argument parsing. You can write a test script that
    # will run for 10 minutes or 1 hour based on command-line parameters, and the test could set an appropriate
    # timeout for either case.
    def get_timeout(self):
        return self.default_timeout_s

    # Subclasses may may override the postprocess() method to do postprocessing of results. This is executed after
    # the test completes, and only if the test was otherwise successful. This is useful if you want to examine
    # the container's stdout or stderr, or the enclave-os.log file to further decide if the test has passed or failed.
    # postprocess() should return true if the test should still be considered a success, or raise an exception or
    # return False if the test run should be considered a failure.
    def postprocess(self):
        return True

    def result(self, testcase, result, message=None):
        if message:
            self.info(message)
        self.info('[{:7}] {}.{}'.format(result.title(), self.__class__.__name__, testcase))
        self.results.submit('apps', self.__class__.__name__, testcase, result, message)

    # Re-sign the specified image, passing args as command-line arguments to the signer. Returns the image tag for
    # the converted image.
    #
    # The memory_model parameter is passed through as the --memory-model option to the signer when the signer
    # is run.
    def re_sign_image(self, input_image=None, memory_model='all', signer=None):
        if self.run_args.signer is None:
            raise Exception('--signer option is required for signer tests')
        new_image_name = gen_hostname('resigned')
        if signer is None:
            signer = self.run_args.signer
        signer_cmdline = [
            signer,
            '-output', new_image_name,
            '--isvsvn', str(self.run_args.isvsvn),
            '--isvprodid', str(self.run_args.isvprodid),
            '--key', self.run_args.key,
            '--docker-container', input_image,
        ]
        if memory_model is not None:
            signer_cmdline.extend(['--memory-model', memory_model,])


        print('Re-signing container with command {}'.format(signer_cmdline))

        # We add the image to the image list before we actually re-sign it. This is to handle cases where the test
        # times out immediately after re-signing the image. But if the re-signing fails, it means that the new
        # image won't actually exist. The cleanup code needs to handle this situation.
        all_images.append(new_image_name)
        subprocess.check_call(signer_cmdline, shell=False)

        print('Re-signed image name is {}'.format(new_image_name))
        return new_image_name

class MalborkContainer(object):
    """ Helper for running malbork containers. Instances of MalborkContainer have an
        instance of NativeContainer in self.container. The standard cleanup logic
        for NativeContainers applies to it. """

    retries              = 150
    malbork_version_file = 'malbork_version.txt'
    FEMC_IMAGE_FILE      = 'test_image_name.txt'
    malbork_container    = None
    env_var              = {'SKIP_BUILD_WHITELIST':'true',
                            'USE_LOCAL_AUTH_KEY':'true',
                            'MALBORK_ADMIN_TLS_CERT': '/app/tls-admin/server-cert.pem',
                            'MALBORK_ADMIN_TLS_KEY': '/app/tls-admin/server-key.pem',
                            'AGENT_MANAGER_AUTH_BASIC_TOKEN': ''.join(random.choice('0123456789abcdef') for n in range(32))}
    zone_cert            = None
    node_base_url        = None
    FEMC_URL_ENV_VAR     = 'NODE_AGENT_BASE_URL'
    FEMC_NODE_PORT       = 8042
    FEMC_BACKEND_PORT    = 8040
    mounts = []
    if is_sgx():
        mounts += [
            {
                'target': '/var/run/aesmd/aesm.socket',
                'source': '/var/run/aesmd/aesm.socket',
                'type': 'bind'
            },
        ]
    devices = get_sgx_devices()

    def __init__(self, run_args = None):
        malbork_run_args = SimpleNamespace()
        if run_args != None:
            self.toolserver = run_args.toolserver
            self.security_opt = run_args.security_opt
            malbork_run_args = run_args
        else:
            self.toolserver = None
            self.security_opt = None
            malbork_run_args.keep_containers_on_success  = False
            malbork_run_args.keep_containers_on_failure = False
            malbork_run_args.debug_shell = None
            malbork_run_args.security_opt = None
            malbork_run_args.post_conv_entry_point = None

        # TODO: We may want to have a way to run the malbork container with privileges or in the host pid namespace
        # to make debugging easier.
        malbork_run_args.privileged = False
        malbork_run_args.host_pid_namespace = False

        self.run_args = malbork_run_args
        self.container_impl = NativeContainer


    def start(self):
        if os.environ.get('RUNNING_FROM_MALBORK_CI'):
            return
        image_name = os.path.join(os.environ['MALBORK_BINARIES_BASE_DIR'], os.environ['FEMC_IMAGE_FILE'])
        self.container = None
        image_id = None

        try:
             with open(image_name, 'r') as f:
                name = f.read().strip()
                # ZIRC-5248 Override the manager test container name until MAL-5751 is complete and
                # we have access to ub20 binaries which have the fix for MAL-5599
                name = "513076507034.dkr.ecr.us-west-1.amazonaws.com/enclaveos/manager-test:1.41.2074"
                docker_client = docker.from_env()
                image_id = docker_client.images.get(name).id
                if image_id is None:
                    print ('Error finding malbork container path & name \n')
                    return None
        except:
            pass

        try:
            remove_ignore_nonexistent(self.malbork_version_file)
            # malbork ports
            ports = {
            self.FEMC_NODE_PORT: None,
            self.FEMC_BACKEND_PORT: None
            }

            image_reg_split = name.rsplit('/', 1)
            registry = image_reg_split[0]
            image_version_split = image_reg_split[1].split(':', 1)
            image = image_version_split[0]
            version = image_version_split[1]

            self.container = self.container_impl(image, registry=registry, image_version=version, run_args=self.run_args, ports=ports, manifest_env=self.env_var)
            extra_args=dict()
            extra_args['mounts'] = self.mounts
            extra_args['devices'] = self.devices
            self.container.prepare(extra_args)
            self.container.run()
            node_port = self.container.get_port_mapping(self.FEMC_NODE_PORT)
            url_path='http://localhost:{}/v1'.format(node_port)
            version_url = url_path + '/sys/version'
            resp = None
            exception_str='Unable to connect to Malbork container: {}'.format(self.container.container.name)
            for i in range(self.retries):
                try:
                    resp = requests.get(version_url)
                    if (resp.status_code == 200):
                        retval = 0
                        break
                    else:
                        retval = -1
                        print('resp status is {}'.format(resp.status_code))
                except Exception as e:
                    retval = -1;
                    if (i >= (self.retries - 1)):
                        exception_str += 'Trial - {} : {} '.format(i, e)
                        raise TestException(exception_str)

                time.sleep(2)

            if retval != 0:
                if resp is not None:
                    exception_str += 'response is {} '.format(resp)
                raise TestException(exception_str)
            else:
                print('Connected to malbork container');
        except:
            raise
            print('Error starting malbork container\n')
            return None
        finally:
            remove_ignore_nonexistent(self.malbork_version_file)

        self.node_base_url = 'http://{}:{}/v1'.format(self.container.get_my_ip(), self.FEMC_NODE_PORT)
        self.env_var['NODE_AGENT_BASE_URL']=self.node_base_url
        self.env_var['SKIP_BUILD_WHITELIST']='true'

    def get_env_var(self):
        if os.environ.get('RUNNING_FROM_MALBORK_CI'):
            self.env_var['SKIP_BUILD_WHITELIST']='true'
        return self.env_var

    def stop(self):
        if os.environ.get('RUNNING_FROM_MALBORK_CI'):
            return
        if (self.container != None):
            self.container.stop()
        self.env_var={}

    def get_node_base_url(self):
        if os.environ.get('RUNNING_FROM_MALBORK_CI'):
            MANAGER_HOST = get_my_ip()
            if not os.environ.get('MANAGER_AGENT_PORT'):
                MANAGER_AGENT_PORT = '6010'
            else:
                MANAGER_AGENT_PORT = os.environ.get('MANAGER_AGENT_PORT')

            self.node_base_url = "http://" + MANAGER_HOST + ":" + MANAGER_AGENT_PORT + "/v1"

        return self.node_base_url

    def get_heartbeat_count(self, app_list=None):
        if self.container:
            # hard code the location of client certificate, there isn't any environment variable set otherwise
            heartbeat_count_query = "cockroach sql --certs-dir /app/db/certs --database malbork -e 'select app_node.message_count, build.mrenclave, app.name from app_node, build, app where app_node.build_id = build.build_id and build.app_id = app.app_id' --format csv"
            heartbeat_count = self.container.container.exec_run(heartbeat_count_query).output
            heartbeat_count_data = heartbeat_count.decode("utf-8").strip().split("\n")
            heartbeat_count_data = csv.reader(heartbeat_count_data, delimiter=",")
            heartbeat_count_list = []
            firstLine = True
            for row in heartbeat_count_data:
            # Skip the first line which contains column names
                if firstLine:
                    firstLine = False
                    continue
                message_count, mrenclave, app_name = row
                heartbeat_count_obj = {
                    "mrenclave": mrenclave,
                    "message_count": int(message_count),
                    "app_name": app_name
                }
                if (app_list is None or app_name in app_list):
                    heartbeat_count_list.append(heartbeat_count_obj)
            return heartbeat_count_list

    def get_zone_cert(self):
        if os.environ.get('RUNNING_FROM_MALBORK_CI'):
            if not os.environ.get('MANAGER_HOST'):
                MANAGER_HOST = 'localhost'
            else:
                MANAGER_HOST = os.environ.get('MANAGER_HOST')

            if not os.environ.get('MANAGER_PORT'):
                MANAGER_PORT = '6060'
            else:
                MANAGER_PORT = os.environ.get('MANAGER_PORT')

            return get_zone_cert_local_malbork(MANAGER_HOST, MANAGER_PORT)

        if (self.zone_cert is not None):
            return self.zone_cert
        elif (self.container is not None):
            get_zone_cert_url_path='https://{}:{}/v1/zones'.format(self.container.get_my_ip(), self.FEMC_BACKEND_PORT)
            zone_cert_response = None
            exception_str='Unable to get zone certificate from container {}: '.format(self.container.container.name)

            for i in range(10):
                try:
                    zone_cert_response = requests.get(get_zone_cert_url_path, verify=False)
                    if (zone_cert_response.status_code == 200):
                        retval = 0
                    else:
                        retval = -1
                        print('resp status is {}'.format(zone_cert_response.status_code))
                    break
                except Exception as e:
                   retval = -1;
                   if (i >= (10 - 1)):
                       exception_str += '{} '.format(e)
                       raise TestException(exception_str)

                time.sleep(1)

            if retval != 0:
                if zone_cert_response is not None:
                    exception_str += 'response is {} '.format(zone_cert_response)
                raise TestException(exception_str)
            else:
                print('Manager certificate was successfully obtained');

            zone_cert = (zone_cert_response.json())['certificate']
            return zone_cert

class EMContainer(object):
    """ Helper for running EM containers. Instances of Container have an
        instance of NativeContainer in self.container. The standard cleanup logic
        for NativeContainers applies to it. """

    retries         = 150
    em_container    = None
    zone_cert       = None
    node_base_url   = None
    EM_NODEAGENT_URL_ENV_VAR = 'NODE_AGENT_BASE_URL'
    EM_NODE_PORT    = 9092
    EM_BACKEND_PORT = 9090
    mounts = [
        {
            'target': '/var/run/aesmd/aesm.socket',
            'source': '/var/run/aesmd/aesm.socket',
            'type': 'bind'
        },
    ]
    devices = get_sgx_devices()

    def __init__(self, run_args = None):
        em_run_args = SimpleNamespace()
        if run_args != None:
            self.toolserver = run_args.toolserver
            self.security_opt = run_args.security_opt
            em_run_args = run_args
        else:
            self.toolserver = None
            self.security_opt = None
            em_run_args.keep_containers_on_success  = False
            em_run_args.keep_containers_on_failure = False
            em_run_args.debug_shell = None
            em_run_args.security_opt = None
            em_run_args.post_conv_entry_point = None

        em_run_args.privileged = False
        em_run_args.host_pid_namespace = False
        self.run_args = em_run_args
        self.container_impl = NativeContainer


    def start(self):
        # TODO: Get the image name from file/malbork-binaries once we have a production image
        image_name = '513076507034.dkr.ecr.us-west-1.amazonaws.com/development-images/malbork/cai:eos'
        self.container = None
        image_id = None

        try:
            docker_client = docker.from_env()
            image_id = docker_client.images.get(image_name).id
            if image_id is None:
                print ('Error finding EM container path & name \n')
                return None
        except:
            pass

        try:
            ports = {
                self.EM_NODE_PORT: None,
                self.EM_BACKEND_PORT: None
            }

            image_reg_split = image_name.rsplit('/', 1)
            registry = image_reg_split[0]
            image_version_split = image_reg_split[1].split(':', 1)
            image = image_version_split[0]
            version = image_version_split[1]

            self.container = self.container_impl(image, registry=registry, image_version=version, run_args=self.run_args, ports=ports)
            extra_args=dict()
            extra_args['mounts'] = self.mounts
            extra_args['devices'] = self.devices
            extra_args['entrypoint'] = '/bin/bash /root/start_corvin_server.sh'
            self.container.prepare(extra_args)
            self.container.copy_file('app.json','/root')
            self.container.copy_file('build.json','/root')
            self.container.copy_file('start_corvin_server.sh','/root')
            self.container.run()

            print('Starting EM test of image id {} ...'.format(image_id))
            node_port = self.container.get_port_mapping(self.EM_NODE_PORT)
            url_path='http://localhost:{}/v1'.format(node_port)
            version_url = url_path + '/sys/version'
            resp = None
            exception_str='Unable to connect to EM container: '
            for i in range(self.retries):
                try:
                    resp = requests.get(version_url)
                    if (resp.status_code == 200):
                        retval = 0
                        break
                    else:
                        retval = -1
                        print('resp status is {}'.format(resp.status_code))
                except Exception as e:
                    retval = -1;
                    if (i >= (self.retries - 1)):
                        exception_str += 'Trial - {} : {} '.format(i, e)
                        raise TestException(exception_str)

                time.sleep(8)
            # TODO Remove delay after fixing https://fortanix.atlassian.net/browse/MAL-199
            time.sleep(20)
            if retval != 0:
                if resp is not None:
                    exception_str += 'response is {} '.format(resp)
                raise TestException(exception_str)
            else:
                print('Connected to EM container');
        except:
            print('Error starting EM container\n')
            raise
            return None

        self.node_base_url = 'http://{}:{}/v1'.format(self.container.get_my_ip(), self.EM_NODE_PORT)

    def get_node_base_url(self):
        return self.node_base_url

    def get_config_id(self):
        if (self.container != None):
            iterations = 15
            sleep_time = 2
            configID = ""
            print("Looking at logs of EM container" +
                " in interval of " + str(sleep_time) + "s with at max "
                + str(iterations) + " iterations\n")

            for ii in range(iterations):
                print("Going to wait for next " + str(sleep_time) + "sec [" + str(ii) + "/" + str(iterations) + "]\n")
                time.sleep(sleep_time)

                substr = "Please start enclave os application with following config id:"
                if re.search(substr,str(self.container.logs())):
                    configID_list = str(self.container.logs()).split(substr,1)
                    if (len(configID_list) == 2):
                        configID = configID_list[1]
                        configID = configID[1:65]
                    break
        return str(configID)




# Handler for the sigalarm that gets generated when our alarm fires.
def timeout_handler(signum, frame):
    print('\n\n\n***** TEST FAILURE: TIMEOUT ****')
    raise TimeoutException('Test timed out')

def term_handler(signum, frame):
    print('\n\n\n***** TEST FAILURE: TERMINATED ****')
    # Raises SystemExit exception
    sys.exit(128 + signum)

# Set an alarm to time out the test. Automatically increase the timeout by a constant factor if we're running in SGX.
def arm_timeout(timeout):
    signal.signal(signal.SIGALRM, timeout_handler)
    if is_sgx():
        timeout *= SGX_TIMEOUT_SCALE
    signal.alarm(timeout)

def global_cleanup(status=True):
    # We have to be slightly tricky here. Calling container.stop may remove the container from all_containers.
    # If all_containers gets modified while we're iterating over it, our iteration may not reach every container
    # in the list. Since all_containers will be short (as of this writing, the biggest test only uses 3 containers),
    # we just make a copy of the list before we start iterating. There are more efficient (but more complicated)
    # solutions if your list is longer.
    # We stop the containers in reverse order to handle cases like an APP sending heartbeat to manager-test.
    # In these tests manager-test container would launch first and should get stopped last, so that we do not have any
    # heartbeat failures in the application's logs.

    success = True
    cleanup_tar_files_during_copying()
    for container in reversed(list(all_containers)):
        # Here, we reset the alarm that we use for generating timeouts. If the rest of the test took just a bit less
        # time than the overall timeout, the original timeout could expire while we're trying to stop the test containers.
        # We want to avoid that because it will result in container tests being left around after the test run completes.
        # There is still a potential race for the timeout expiring just after the test failed due to some other exception.
        # Fixing that probably requires wrapping the test execution in yet another try/except block so we can catch
        # any timeout exceptions that happen in that window.
        signal.alarm(DEFAULT_STOP_TIMEOUT)
        try:
            print('global_cleanup() stopping container {}'.format(container.name))
            logfile = container.stop(remove=True, status=status)
            if logfile:
                panic = check_zircon_log_for_panic(logfile)
                if panic:
                    if container.zircon_panic_expected:
                        print('WARNING: log file indicated a zircon panic')
                        print('This is expected for this test case ({})\n'.format(container.zircon_panic_expected))
                    else:
                        success = False

        except Exception as e:
            traceback.print_exc()
            print('Exception trying to stop container: {}'.format(e))
            success = False
    destroy_network()

    docker_client = docker.from_env()
    for image in all_images:
        try:
            print('Removing image {}'.format(image))
            docker_client.images.remove(image=image, force=True, noprune=False)
        except Exception:
            # Ignore errors on deleting images, but warn.
            print('Deleting image {} failed (perhaps image did not get created?)\n'.format(image))

    print('global_cleanup() returning {}'.format(success))
    return success

CONTAINER_ENVS = {
    'native': NativeContainer,
    'zircon': ZirconDockerContainer,
    'native-k8s': NativeKubeContainer,
    'k8s': ZirconKubeContainer,
    'nitro': NitroDockerContainer,
}

DEFAULT_CONTAINER_ENV = 'zircon'

def main(klass, stdout_to_console=False):
    parser = argparse.ArgumentParser(description='Run an application test.')
    parser.add_argument('--container-env', choices=CONTAINER_ENVS.keys(), default=DEFAULT_CONTAINER_ENV,
            help='environment for running the application')
    parser.add_argument('--manifest-env', action='append', default=[], help='environment variables at build time (included on manifest)')
    parser.add_argument('--manifest-option', action='append', default=[], help='Additional options to add to app\'s manifest file (in key=value format)')
    parser.add_argument('--benchmark-duration', type=int, default=20,
            help='Input time in seconds to run benchmarks for. Currently supports Nginx WRK and Mariadb sysbench. Default value is 20.')
    parser.add_argument('--mariadb-workload', choices=['complex', 'readonly'], default='complex',
            help='choosing complex workload will simulate a oltp workload whereas choosing readonly will only run select queries. Default=complex')
    parser.add_argument('--zircon-debug', '--graphene-debug', action='store_true', default=False,
            help='enable zircon debug output (must also be supported by build, and may confuse pexpect)')
    parser.add_argument('--image', action='append', help='Image tag to use for this test (may be passed more than once)')
    # TODO: Rename --toolserver option to --converter.
    parser.add_argument('--toolserver', help='Path to toolserver command')
    parser.add_argument('--signer', help='Path to enclaveos-signer (only needed for signer tests)')
    parser.add_argument('--installer', help='path to enclaveos installer')
    parser.add_argument('--key', help='Path to signing key in PEM format')
    parser.add_argument('--isvsvn', help='Security release number', type=int, default=0, dest='isvsvn')
    parser.add_argument('--isvprodid', help='Product id', type=int, default=0, dest='isvprodid')
    parser.add_argument('--keep-converted-images', action='store_true',
                        help='Keep the converted image(s) after running test')
    parser.add_argument('--converted-image', action='store',
                        help='Run test using image converted from previous run instead of converting image again')
    parser.add_argument('--toolserver-verbose', action='store_true',
                        help='Enable verbose debugging for the toolserver')
    parser.add_argument('--security-opt', action='append',
                        help='Additional security options to run with')

    parser.add_argument('--debug-shell', action='store_true',
                        help='Start the converted container with a shell for debugging, instead of running application')

    parser.add_argument('--keep-containers-on-success', action='store_true', default=False,
                        help='Keep the test\'s container after a successful run')
    parser.add_argument('--keep-containers-on-failure', action='store_true', default=False,
                        help='Keep the test\'s container after a failing run')
    parser.add_argument('--post-conv-entry-point', action='store_true',
                        help='Override the entry point of the image after conversion.')
    parser.add_argument('--no-results-db', action='store_true',
                        help='Do not write test results to the database')

    parser.add_argument('--test-arg', action='append', help='arguments to pass to the test')

    parser.add_argument('--allow-cmdline-arguments', action='store_true', default=False,
                        help='Allow commands to be run at runtime')

    # The following options apply to the test container(s) run by the script
    parser.add_argument('--privileged', action='store_true', help='Run test containers with docker --privileged option')
    parser.add_argument('--host-pid-namespace', action='store_true', help='Run test containers with docker --host-pid-namespace option')

    run_args = parser.parse_args()
    test_arg_list = run_args.test_arg or []

    del run_args.test_arg

    if run_args.converted_image:
        run_args.keep_converted_images = True

    if os.environ.get('RUNNING_FROM_MALBORK_CI'):
        scriptDirectory = os.path.dirname(os.path.realpath(__file__))
        home_dir_path = '/'.join(scriptDirectory.split('/')[:-3])
        run_args.toolserver = os.path.join(home_dir_path, 'opt', 'fortanix', 'converter', 'bin', 'build_app.py')
        run_args.installer = os.path.join(home_dir_path, 'enclave-os_{}_amd64.deb'.format(os.environ.get('PRODUCT_VERSION')))
        run_args.key = os.environ.get('ENCLAVEOS_SIGNING_KEY')

    test = klass(run_args, test_arg_list)

    global failed

    # Run the test and interpret the results. We support several kinds of tests:
    #  1. The test can return True/False from the run method to indicate success/failure
    #  2. The test can define a postproces method. In this case both run() and
    #     postprocess() must return True
    #  3. The test can submit (possibly multiple) results directly by calling
    #     `TestApp.result()`.
    results = TestResults(use_db=not run_args.no_results_db)
    try:
        signal.signal(signal.SIGTERM, term_handler)
        signal.signal(signal.SIGQUIT, term_handler)
        arm_timeout(test.get_timeout())
        if (not test.run()) or (test.results.count() != 0 and not test.results.all_pass()):
            failed = True
            TestApp.info('\n\n==== Test {} FAILED ====\n'.format(klass.__name__))
            # Also submit a result if the test wasn't logging individual results.
            if test.results.count() == 0:
                results.submit('apps', klass.__name__, 'test', 'FAILED', None)
            exit(1)
        if (not test.postprocess()) or (test.results.count() != 0 and not test.results.all_pass()):
            failed = True
            TestApp.info('\n\n==== Test {} FAILED in postprocessing ====\n'.format(klass.__name__))
            raise TestException('postprocessing failed')
    except Exception as e:
        failed = True
        TestApp.info('\n\n==== Test {} FAILED ====\n'.format(klass.__name__))
        if test.results.count() == 0:
            results.submit('apps', klass.__name__, 'test', 'ERROR',
                           'Test exception: {}'.format(e))
        raise
    finally:
        success = global_cleanup(not failed)
        if (stdout_to_console):
            os.system('cat logs/*stdout*')


    if not success:
        TestApp.info('\n\n==== Test {} FAILED ====\n'.format(klass.__name__))
        TestApp.info('Failure in global cleanup\n')
        exit(1)

    TestApp.info('\n\n==== Test {} PASSED ====\n'.format(klass.__name__))
    if test.results.count() == 0:
        results.submit('apps', klass.__name__, 'test', 'PASSED', None)
    exit(0)


# Function to return local IP address. This IP will
# be utilized to connect from within the converted
# container to Malbork Server and obtain a certificate
def get_my_ip():
    import socket
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        s.connect(('10.255.255.255', 1))
        local_ip = s.getsockname()[0]
        s.close()
    except socket.error as e:
        raise TestException(e)
    return local_ip



# Function to return IP Address of a local interface
# that is able to connect to a given IP. This function
# is useful to fetch the correct Host IP that is able
# to connect to the docker running on it
def get_ip_address(ip):
    import socket
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        s.connect((ip, 80))
        return s.getsockname()[0]
    except socket.error as e:
        print('Unable to connect to IP')
        s.close()
        raise TestException(e)


# Function to return the Zone Certificate
# from malbork server running locally
# on a given host and port
def get_zone_cert_local_malbork(host, port):
    get_zone_cert_url_path='https://{}:{}/v1/zones'.format(host, port)
    zone_cert_response = None
    exception_str='Unable to get zone certificate: '

    for i in range(10):
        try:
            zone_cert_response = requests.get(get_zone_cert_url_path, verify=False)
            if (zone_cert_response.status_code == 200):
                retval = 0
            else:
                retval = -1
                print('resp status is {}'.format(zone_cert_response.status_code))
            break
        except Exception as e:
            retval = -1;
            if (i >= (10 - 1)):
                exception_str += '{} '.format(e)
                raise TestException(exception_str)

        time.sleep(1)

    if retval != 0:
        if zone_cert_response is not None:
            exception_str += 'response is {} '.format(zone_cert_response)
        raise TestException(exception_str)
    else:
        print('Manager certificate was successfully obtained');

    zone_cert = (zone_cert_response.json())['certificate']
    return zone_cert
